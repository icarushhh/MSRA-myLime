{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUQrjOgyoyGl",
        "outputId": "cd52e5b9-b326-4d96-fbf1-7c72b556a946"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 挂载 Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnR3m0iTpL1s",
        "outputId": "608e18c2-1dda-4e93-fdd8-97457d8cf13d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# 加载模型和分词器\n",
        "dir = '/content/drive/MyDrive/Bert_Finetune/Bert fine-tuned on a subset of sentiment140/model_save'\n",
        "model = BertForSequenceClassification.from_pretrained(dir)\n",
        "tokenizer = BertTokenizer.from_pretrained(dir)\n",
        "class_names = ['negative','postive']"
      ],
      "metadata": {
        "id": "NzSkSZx_pPPq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import my_lime\n",
        "import myTextExplainer\n",
        "import lime_base\n",
        "import importlib\n",
        "importlib.reload(my_lime)\n",
        "importlib.reload(myTextExplainer)\n",
        "importlib.reload(lime_base)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUt9sT3oRnE9",
        "outputId": "e40ea7c2-92f1-455a-d753-79201c261464"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'lime_base' from '/content/lime_base.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from myTextExplainer import *\n",
        "\n",
        "def predictor(texts):\n",
        "  outputs = model(**tokenizer(texts, return_tensors=\"pt\", padding=True))\n",
        "  probas = F.softmax(outputs.logits).detach().numpy()\n",
        "  return probas\n",
        "\n",
        "string = \"I really want to apply for a au pair job, but I'm so fucking scared of change, that I keep overthinking everything.\"\n",
        "\n",
        "explainer = TextExplainer()\n",
        "\n",
        "explainer.explain_string_by_lime(string, predictor, num_features=5, num_samples=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l5-x44cpZlj",
        "outputId": "71afde67-6fb0-43db-8b9b-77558de6d630"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-75-e85520d153ee>:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  probas = F.softmax(outputs.logits).detach().numpy()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "explainer.show_as_list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5oYZzPN8oYw",
        "outputId": "bd62bbb1-dd43-4d69-8034-fa90f33c97c7"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('scared', -0.09749610209588747), ('everything', -0.09687039736035921), ('overthinking', -0.08582481573747666), ('that', -0.07400962232352232), ('fucking', -0.06115556695916465)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "explainer.explain_string_by_mylime(string, predictor, num_features=5, num_samples=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ_zMTkJ8plh",
        "outputId": "712391e2-0101-4850-fda1-39aecc80a9c7"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-75-e85520d153ee>:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  probas = F.softmax(outputs.logits).detach().numpy()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "explainer.show_as_list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCM1l2AaN9uY",
        "outputId": "98af2955-9c57-491c-8183-2f6d2fabd3a5"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('scared', -0.07931565767091334), ('overthinking', -0.06448008970269112), ('want', -0.05817741273839967), ('but', -0.04582327703241681), ('to', -0.030972555801291123)]\n"
          ]
        }
      ]
    }
  ]
}